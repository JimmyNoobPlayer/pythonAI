Every value that is used by a neuralnet or a trainer should be in a nnlist, a very simple subclass of a python list, which has a method to switch itself into a numpy column vector, which is used in the internal calculations of the neural net.

np.random.uniform(0.0, 1.0, [self.length, 1])
np.array([[x1],[x2],[x3]...])

(should be in neuralnetutil.py and be used throughout the whole project)




The trainer objects are iterators that return tuples of two nnlists, the first representing the input of the neural net, the second representing the output. Trainers should never raise StopIteration.

Visualizers are a bit more difficult. They need to be able to visualize the error of a neural net, which means the input must have some way of being visualized (such as being uniformly spread over a domain in two dimensions.... in many implementations, visualization could be impossible or not very helpful. What kinds of visualizations are needed? This is a big question.

The most essential kind of visualization is the error curve, plotting the error of the neural net (as measured by a trainer) as stochastic gradient descent is run on that neural net with that trainer.


The heart of this package is neuralnet.net in the file neuralnet.py, an object that holds all the weights and biases in many layers to represent a neural net (which is a thing capable of approximating any arbitrary function to a reasonable degree of accuracy -depending on how big the neural net is- and has a backpropagation algorithm capable of minimizing the error of the net to match any function (which in practice is commonly a list of human-labeled data, or output from a crazy Monte-Carlo Tree Search gameplay system.)


trainer.py...


visualizer.py...


